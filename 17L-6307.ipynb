{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler\n",
    "\n",
    "This notebook contains started code structure for creating a crawler on single machine\n",
    "\n",
    "**Author:** Noshaba Nasir\n",
    "\n",
    "**Date:** 26/3/2021\n",
    "\n",
    "**Updated by:** <Write your name and Roll number here>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import codecs \n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "from queue import Queue\n",
    "from datetime import datetime,timedelta\n",
    "import threading, queue, time, urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import request\n",
    "from urllib.parse import urlsplit\n",
    "from urllib.parse import urlparse\n",
    "from urllib import parse\n",
    "from urllib import robotparser\n",
    "# Add any library to be imported here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawler Parameters\n",
    "BACKQUEUES= 3\n",
    "THREADS= BACKQUEUES*3\n",
    "FRONTQUEUES= 5\n",
    "WAITTIME= 15 ; # wait 15 seconds before fetching URLS from \n",
    "FILENAME = 1\n",
    "\n",
    "# Add any other global parameters here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRONTIER\n",
    "\n",
    "Frontier should use the Mercator frontier design as discussed in lecture.\n",
    "\n",
    "Preferably it should be a class and should have the given functions.\n",
    "\n",
    "*prioritizer* function is a stub right now, it will return a random number  between 1 to f for given URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_diff_in_Seconds(dt2, dt1):\n",
    "    timedelta = dt2 - dt1\n",
    "    return timedelta.days * 24 * 3600 + timedelta.seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class frontier:\n",
    "# add the code for frontier here\n",
    "# should have functions __init__, get_URL, add_URLs, add_to_backqueue\n",
    "\n",
    "    def __init__(self,FQueue,BQueue,TimeStamp,CrawledURL,TotalUrlCrawled):\n",
    "        self.FQueue = FQueue\n",
    "        self.BQueue = BQueue\n",
    "        self.TimeStamp = TimeStamp\n",
    "        self.CrawledURL = CrawledURL\n",
    "        self.TotalUrlCrawled = TotalUrlCrawled\n",
    "        seed_urls = [\n",
    "            \"https://docs.oracle.com/en/\",\n",
    "            \"https://www.oracle.com/corporate/\",\n",
    "            \"https://en.wikipedia.org/wiki/Machine_learning\",\n",
    "            \"https://www.csie.ntu.edu.tw/~cjlin/libsvm/index.html\",\n",
    "            \"https://docs.oracle.com/middleware/jet210/jet/index.html\",\n",
    "            \"https://en.wikipedia.org/w/api.php\",\n",
    "            \"https://en.wikipedia.org/api/\",\n",
    "            \"https://en.wikipedia.org/wiki/Weka_(machine_learning)\"\n",
    "        ]\n",
    "        for i in range(len(seed_urls)):\n",
    "            priority = random.randint(0,4)\n",
    "            FQueue[priority].put(seed_urls[i])\n",
    "        return\n",
    "    #Adds URL to corresponding Front Queue after giving it prioirity using prioritizer function \n",
    "    def add_URLs(self,URL,FRONTQUEUES):\n",
    "        priority = self.prioritizer(URL,FRONTQUEUES)\n",
    "        #print(FQueue[priority].qsize())\n",
    "        FQueue[priority].put(URL)\n",
    "        #print(FQueue[priority].qsize())\n",
    "        return\n",
    "    \n",
    "    def prioritizer(self,URL,f):\n",
    "        return random.randint(0,f-1)\n",
    "    \n",
    "    # Returns BackQueue which has not been accsessed for the longest time\n",
    "    def get_URL(self):\n",
    "        CurrentDateTime=datetime.now()\n",
    "        TimeDiff =[]\n",
    "        TimeDiff = [0 for i in range(3)] \n",
    "#         print(\"TIMESTAMP\")\n",
    "#         print(self.TimeStamp[1])\n",
    "        for i in range(3):\n",
    "            TimeDiff[i]=date_diff_in_Seconds(CurrentDateTime, self.TimeStamp[i])\n",
    "        \n",
    "        MinBackQueue = TimeDiff.index(max(TimeDiff))\n",
    "        print(TimeDiff)\n",
    "        return MinBackQueue\n",
    "        \n",
    "    def add_to_backqueue(self):\n",
    "        for i in range(3):\n",
    "            if(BQueue[i].qsize() == 0): #BackQueue is empty\n",
    "                FrontQueue = np.random.choice(np.arange(0, 5), p=[(1/15), (2/15), (3/15), (4/15), (5/15)])\n",
    "                while(FQueue[FrontQueue].qsize() == 0):\n",
    "                    FrontQueue = np.random.choice(np.arange(0, 5), p=[(1/15), (2/15), (3/15), (4/15), (5/15)])\n",
    "                URL= FQueue[FrontQueue].queue[0]\n",
    "                print(URL)\n",
    "                parts = urlsplit(URL)\n",
    "                base_url = \"{0.scheme}://{0.netloc}\".format(parts)\n",
    "                \n",
    "                flag = 0\n",
    "                for j in range(3):\n",
    "                    if BQueue[j].qsize() != 0:\n",
    "                        Backqueue_URL=BQueue[j].queue[0]\n",
    "                        Backqueue_parts = urlsplit(Backqueue_URL)\n",
    "                        Backqueue_base_url = \"{0.scheme}://{0.netloc}\".format(Backqueue_parts)\n",
    "                        \n",
    "                        if Backqueue_base_url == base_url : # Backqueue baseURL match base_url in another queue\n",
    "                            flag = 1\n",
    "                            BQueue[j].put(FQueue[FrontQueue].get()) #URL to backqueue as base url match\n",
    "                if(flag == 0):\n",
    "                    #print(FQueue[FrontQueue].qsize())\n",
    "                    BQueue[i].put(FQueue[FrontQueue].get())\n",
    "                    #print(FQueue[FrontQueue].qsize())\n",
    "                    return True\n",
    "                else:\n",
    "                    #print(\"FLAG\")\n",
    "                    return False\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FQueue = [ Queue() for i in range(FRONTQUEUES) ]\n",
    "BQueue = [ Queue() for i in range(BACKQUEUES) ]\n",
    "TimeStamp = []\n",
    "TimeStamp = [datetime.now() - timedelta(seconds=15) for i in range(3)] \n",
    "CrawledURL=set()\n",
    "TotalUrlCrawled = 0\n",
    "\n",
    "Object = frontier(FQueue,BQueue,TimeStamp,CrawledURL,TotalUrlCrawled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILTER URLS\n",
    "\n",
    "Filter the URLS that are in robots.txt files of server and the have been already processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code for filtering urls here \n",
    "def filter_robot(base_URL, Crawled_URLs):\n",
    "    Filtered_URLs = []\n",
    "    size = len(Crawled_URLs)\n",
    "    for i in range (size):\n",
    "        parser = robotparser.RobotFileParser()\n",
    "        parser.set_url(parse.urljoin(base_URL, 'robots.txt'))\n",
    "        parser.read()\n",
    "        flag = parser.can_fetch(\"*\",Crawled_URLs[i])\n",
    "        if(flag == True):\n",
    "            Filtered_URLs.append(Crawled_URLs[i])\n",
    "    return Filtered_URLs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------Write any other codes/data require to run the crawler above this cell-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thread task\n",
    "def crawler_thread_task():\n",
    "    \n",
    "    # If any backqueue is empty populate it using add to backqueue()\n",
    "    lock.acquire()\n",
    "    while(Object.BQueue[0].qsize() == 0 or Object.BQueue[1].qsize() == 0 or Object.BQueue[2].qsize() == 0 ):\n",
    "        Object.add_to_backqueue()\n",
    "    lock.release()\n",
    "    # get Back queue number whose last access time is maximum\n",
    "    BQueueNumber = Object.get_URL()\n",
    "\n",
    "   \n",
    "    #BackQueue with least time selected is checked if the server was reached less than 15 seconds ago if yes trhe thread sleeps\n",
    "    CurrentDateTime = datetime.now()\n",
    "    LastAccessed = date_diff_in_Seconds(CurrentDateTime, Object.TimeStamp[BQueueNumber])\n",
    "    while(LastAccessed < 15):\n",
    "        time.sleep(1)\n",
    "        CurrentDateTime=datetime.now()\n",
    "        LastAccessed = date_diff_in_Seconds(CurrentDateTime, Object.TimeStamp[BQueueNumber])\n",
    "        \n",
    "    lock.acquire()\n",
    "    #URL which needs to be processed\n",
    "    ProcessURL = BQueue[BQueueNumber].get() \n",
    "    #Update last access time of back queue\n",
    "    Object.TimeStamp[BQueueNumber] = datetime.now()\n",
    "    print(\"BackQueue Number:\")\n",
    "    print(BQueueNumber)\n",
    "    lock.release()\n",
    "    \n",
    "    print(\"URL PROCESSING NOW\")\n",
    "    print(ProcessURL)\n",
    "    ProcessedURLs.add(ProcessURL)\n",
    "\n",
    "    soup = BeautifulSoup(requests.get(url=ProcessURL,).text, 'lxml')\n",
    "#     with open(\"output1.html\", \"a\", encoding='utf-8') as file:\n",
    "#         file.write(str(soup))\n",
    "    global FILENAME\n",
    "    f = codecs.open('example'+str(FILENAME)+'.xml','a','utf-8')\n",
    "    f.write(str(soup))\n",
    "    f.close()\n",
    "    FILENAME = FILENAME+1\n",
    "    parts = urlsplit(ProcessURL)\n",
    "    base = \"{0.netloc}\".format(parts)\n",
    "    strip_base = base.replace(\"www.\", \"\")\n",
    "    base_url = \"{0.scheme}://{0.netloc}\".format(parts)\n",
    "    path = ProcessURL[:ProcessURL.rfind('/')+1] if '/' in parts.path else ProcessURL\n",
    "    for link in soup.find_all('a'):\n",
    "        anchor = link.attrs[\"href\"] if \"href\" in link.attrs else ''\n",
    "        if anchor.startswith('/'):\n",
    "            Crawled_URLs.append(base_url + anchor) #relative URL\n",
    "        elif anchor.startswith('http'):\n",
    "            Crawled_URLs.append(path + anchor) #absolute URL\n",
    "    Filtered_URL =[]\n",
    "    #Only permitted URL returned\n",
    "    Filtered_URL = filter_robot(base_url,Crawled_URLs)\n",
    "    UniqueCrawled =set(Filtered_URL) # Only unique crawled URLs\n",
    "    # ensures the URL already processed do not get added to the front queue again\n",
    "    FinalURL = UniqueCrawled.difference(ProcessedURLs)  # Add to frontier as Filtered and Duplicates removed\n",
    "    \n",
    "    print(ProcessedURLs)\n",
    "    print(\"Crawled\")\n",
    "    print(len(Crawled_URLs))\n",
    "    print(\"Filtered\")\n",
    "    print(len(Filtered_URL))\n",
    "    print(\"FINALURLS\")\n",
    "    print(len(FinalURL))\n",
    "    lock.acquire()\n",
    "    Object.TotalUrlCrawled = Object.TotalUrlCrawled+len(FinalURL)\n",
    "    lock.release()\n",
    "    f = open(\"CrawledURLs.txt\", \"a\")\n",
    "    for i in range(len(FinalURL)):\n",
    "        url = FinalURL.pop()\n",
    "        Object.add_URLs(url,FRONTQUEUES)\n",
    "        Object.CrawledURL.add(url)\n",
    "        f.write(url+\"\\n\")\n",
    "    f.close()\n",
    "    return # define individual crawler thread's function here as studies in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reapeatFunction():\n",
    "    while Object.TotalUrlCrawled < 1000:\n",
    "        crawler_thread_task()\n",
    "        print(\"Number of URL's Crawled \")\n",
    "        print(Object.TotalUrlCrawled)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize every thing \n",
    "lock = threading.Lock()\n",
    "ProcessedURLs= set()\n",
    "Crawled_URLs = []\n",
    "Filtered_URL =[]\n",
    "FinalURL = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Weka_(machine_learning)\n",
      "https://docs.oracle.com/en/\n",
      "https://docs.oracle.com/middleware/jet210/jet/index.html\n",
      "https://en.wikipedia.org/wiki/Machine_learning\n",
      "https://en.wikipedia.org/w/api.php\n",
      "https://www.oracle.com/corporate/\n",
      "[15, 15, 15]\n",
      "BackQueue Number:\n",
      "0\n",
      "URL PROCESSING NOW\n",
      "https://en.wikipedia.org/wiki/Weka_(machine_learning)\n",
      "{'https://en.wikipedia.org/wiki/Weka_(machine_learning)'}\n",
      "Crawled\n",
      "223\n",
      "Filtered\n",
      "199\n",
      "FINALURLS\n",
      "179\n",
      "Number of URL's Crawled \n",
      "179\n",
      "[149, 164, 164]\n",
      "BackQueue Number:\n",
      "1\n",
      "URL PROCESSING NOW\n",
      "https://docs.oracle.com/en/\n",
      "{'https://docs.oracle.com/en/', 'https://en.wikipedia.org/wiki/Weka_(machine_learning)'}\n",
      "Crawled\n",
      "223\n",
      "Filtered\n",
      "223\n",
      "FINALURLS\n",
      "203\n",
      "Number of URL's Crawled \n",
      "382\n",
      "[501, 351, 516]\n",
      "BackQueue Number:\n",
      "2\n",
      "URL PROCESSING NOW\n",
      "https://www.oracle.com/corporate/\n",
      "{'https://www.oracle.com/corporate/', 'https://docs.oracle.com/en/', 'https://en.wikipedia.org/wiki/Weka_(machine_learning)'}\n",
      "Crawled\n",
      "264\n",
      "Filtered\n",
      "264\n",
      "FINALURLS\n",
      "241\n",
      "Number of URL's Crawled \n",
      "623\n",
      "https://www.csie.ntu.edu.tw/~cjlin/libsvm/index.html\n",
      "[716, 567, 215]\n",
      "BackQueue Number:\n",
      "0\n",
      "URL PROCESSING NOW\n",
      "https://en.wikipedia.org/wiki/Machine_learning\n",
      "{'https://www.oracle.com/corporate/', 'https://docs.oracle.com/en/', 'https://en.wikipedia.org/wiki/Weka_(machine_learning)', 'https://en.wikipedia.org/wiki/Machine_learning'}\n",
      "Crawled\n",
      "1570\n",
      "Filtered\n",
      "1455\n",
      "FINALURLS\n",
      "1113\n",
      "Number of URL's Crawled \n",
      "1736\n",
      "Program Terminated\n"
     ]
    }
   ],
   "source": [
    "# start the threads\n",
    "#crawler_thread_task()\n",
    "for i in range(THREADS):\n",
    "    t = threading.Thread(target=reapeatFunction)\n",
    "    t.start()\n",
    "    t.join() \n",
    "print(\"Program Terminated\")     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------------------------------------------------------End of Notebook---------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
